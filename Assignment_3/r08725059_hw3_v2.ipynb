{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.1\n",
    "Download the Adult dataset. Clean up the dataset and create x_train, y_train, x_test, y_test for training feature, training value, test feature, test label. All of these variables should be numpy arrays. Provide summary statistics for your training and test datasets so that TA can verify the correctness of your procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and testing data \n",
    "column_list = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label']\n",
    "train_df = pd.read_csv('adult.data', names=column_list)\n",
    "test_df = pd.read_csv('adult.test', names=column_list, skiprows=1)\n",
    "original_features_list = list(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30152, 103)\n",
      "(15055, 103)\n"
     ]
    }
   ],
   "source": [
    "# remove all rows with missing values. \n",
    "train_df.dropna(axis=0, how='any', inplace=True)\n",
    "test_df.dropna(axis=0, how='any', inplace=True)\n",
    "for col in column_list:\n",
    "    train_df = train_df[train_df[col] != ' ?']\n",
    "    test_df = test_df[test_df[col] != ' ?']\n",
    "\n",
    "# label value will be 1 for '>50K' and 0 otherwise\n",
    "train_df.replace(regex = {'>50K': 1, '<=50K' : 0}, inplace=True)\n",
    "test_df.replace(regex = {'>50K': 1, '<=50K' : 0}, inplace=True)\n",
    "\n",
    "# features with discrete-values be converted to \"1-of-K\" encoding\n",
    "dummy_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "# Include a particular feature value only if this unique value appears more than ten times in the training data.\n",
    "train_v = train_df[dummy_columns]\n",
    "train_df = train_df[train_v.replace(train_v.apply(pd.Series.value_counts)).gt(10).all(1)]\n",
    "test_v = test_df[dummy_columns]\n",
    "test_df = test_df[test_v.replace(train_v.apply(pd.Series.value_counts)).gt(10).all(1)]\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns=dummy_columns)\n",
    "test_df = pd.get_dummies(test_df, columns=dummy_columns)\n",
    "new_features_list = list((train_df.drop(['label'], axis=1)).columns)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Portugal</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30152.000000</td>\n",
       "      <td>3.015200e+04</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "      <td>30152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.440568</td>\n",
       "      <td>1.897916e+05</td>\n",
       "      <td>10.121319</td>\n",
       "      <td>1092.370025</td>\n",
       "      <td>88.266085</td>\n",
       "      <td>40.931348</td>\n",
       "      <td>0.248972</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>0.068553</td>\n",
       "      <td>0.739089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.911880</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.135362</td>\n",
       "      <td>1.056567e+05</td>\n",
       "      <td>2.550204</td>\n",
       "      <td>7407.547899</td>\n",
       "      <td>404.046306</td>\n",
       "      <td>11.979776</td>\n",
       "      <td>0.432425</td>\n",
       "      <td>0.173257</td>\n",
       "      <td>0.252696</td>\n",
       "      <td>0.439139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033562</td>\n",
       "      <td>0.060017</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>0.048469</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>0.023738</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>0.283474</td>\n",
       "      <td>0.046023</td>\n",
       "      <td>0.023030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.376900e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.176248e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.784250e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.376255e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  30152.000000  3.015200e+04   30152.000000  30152.000000  30152.000000   \n",
       "mean      38.440568  1.897916e+05      10.121319   1092.370025     88.266085   \n",
       "std       13.135362  1.056567e+05       2.550204   7407.547899    404.046306   \n",
       "min       17.000000  1.376900e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.176248e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.784250e+05      10.000000      0.000000      0.000000   \n",
       "75%       47.000000  2.376255e+05      13.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week         label  workclass_ Federal-gov  \\\n",
       "count    30152.000000  30152.000000            30152.000000   \n",
       "mean        40.931348      0.248972                0.030976   \n",
       "std         11.979776      0.432425                0.173257   \n",
       "min          1.000000      0.000000                0.000000   \n",
       "25%         40.000000      0.000000                0.000000   \n",
       "50%         40.000000      0.000000                0.000000   \n",
       "75%         45.000000      0.000000                0.000000   \n",
       "max         99.000000      1.000000                1.000000   \n",
       "\n",
       "       workclass_ Local-gov  workclass_ Private  ...  \\\n",
       "count          30152.000000        30152.000000  ...   \n",
       "mean               0.068553            0.739089  ...   \n",
       "std                0.252696            0.439139  ...   \n",
       "min                0.000000            0.000000  ...   \n",
       "25%                0.000000            0.000000  ...   \n",
       "50%                0.000000            1.000000  ...   \n",
       "75%                0.000000            1.000000  ...   \n",
       "max                1.000000            1.000000  ...   \n",
       "\n",
       "       native-country_ Portugal  native-country_ Puerto-Rico  \\\n",
       "count              30152.000000                 30152.000000   \n",
       "mean                   0.001128                     0.003615   \n",
       "std                    0.033562                     0.060017   \n",
       "min                    0.000000                     0.000000   \n",
       "25%                    0.000000                     0.000000   \n",
       "50%                    0.000000                     0.000000   \n",
       "75%                    0.000000                     0.000000   \n",
       "max                    1.000000                     1.000000   \n",
       "\n",
       "       native-country_ Scotland  native-country_ South  \\\n",
       "count              30152.000000           30152.000000   \n",
       "mean                   0.000365               0.002355   \n",
       "std                    0.019097               0.048469   \n",
       "min                    0.000000               0.000000   \n",
       "25%                    0.000000               0.000000   \n",
       "50%                    0.000000               0.000000   \n",
       "75%                    0.000000               0.000000   \n",
       "max                    1.000000               1.000000   \n",
       "\n",
       "       native-country_ Taiwan  native-country_ Thailand  \\\n",
       "count            30152.000000              30152.000000   \n",
       "mean                 0.001393                  0.000564   \n",
       "std                  0.037297                  0.023738   \n",
       "min                  0.000000                  0.000000   \n",
       "25%                  0.000000                  0.000000   \n",
       "50%                  0.000000                  0.000000   \n",
       "75%                  0.000000                  0.000000   \n",
       "max                  1.000000                  1.000000   \n",
       "\n",
       "       native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
       "count                     30152.000000                   30152.000000   \n",
       "mean                          0.000597                       0.911880   \n",
       "std                           0.024426                       0.283474   \n",
       "min                           0.000000                       0.000000   \n",
       "25%                           0.000000                       1.000000   \n",
       "50%                           0.000000                       1.000000   \n",
       "75%                           0.000000                       1.000000   \n",
       "max                           1.000000                       1.000000   \n",
       "\n",
       "       native-country_ Vietnam  native-country_ Yugoslavia  \n",
       "count             30152.000000                30152.000000  \n",
       "mean                  0.002123                    0.000531  \n",
       "std                   0.046023                    0.023030  \n",
       "min                   0.000000                    0.000000  \n",
       "25%                   0.000000                    0.000000  \n",
       "50%                   0.000000                    0.000000  \n",
       "75%                   0.000000                    0.000000  \n",
       "max                   1.000000                    1.000000  \n",
       "\n",
       "[8 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary statistic\n",
    "train_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Portugal</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15055.000000</td>\n",
       "      <td>1.505500e+04</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "      <td>15055.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.769711</td>\n",
       "      <td>1.896046e+05</td>\n",
       "      <td>10.112255</td>\n",
       "      <td>1120.188907</td>\n",
       "      <td>89.071471</td>\n",
       "      <td>40.950714</td>\n",
       "      <td>0.245566</td>\n",
       "      <td>0.030422</td>\n",
       "      <td>0.068615</td>\n",
       "      <td>0.732049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004384</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.915510</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.381046</td>\n",
       "      <td>1.056274e+05</td>\n",
       "      <td>2.558629</td>\n",
       "      <td>7704.274825</td>\n",
       "      <td>406.347469</td>\n",
       "      <td>12.064465</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.171751</td>\n",
       "      <td>0.252807</td>\n",
       "      <td>0.442907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>0.066068</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.044597</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>0.278131</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>0.021559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.349200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.166450e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.779510e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.385810e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>3770.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  15055.000000  1.505500e+04   15055.000000  15055.000000  15055.000000   \n",
       "mean      38.769711  1.896046e+05      10.112255   1120.188907     89.071471   \n",
       "std       13.381046  1.056274e+05       2.558629   7704.274825    406.347469   \n",
       "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.166450e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.779510e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.385810e+05      13.000000      0.000000      0.000000   \n",
       "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
       "\n",
       "       hours-per-week         label  workclass_ Federal-gov  \\\n",
       "count    15055.000000  15055.000000            15055.000000   \n",
       "mean        40.950714      0.245566                0.030422   \n",
       "std         12.064465      0.430437                0.171751   \n",
       "min          1.000000      0.000000                0.000000   \n",
       "25%         40.000000      0.000000                0.000000   \n",
       "50%         40.000000      0.000000                0.000000   \n",
       "75%         45.000000      0.000000                0.000000   \n",
       "max         99.000000      1.000000                1.000000   \n",
       "\n",
       "       workclass_ Local-gov  workclass_ Private  ...  \\\n",
       "count          15055.000000        15055.000000  ...   \n",
       "mean               0.068615            0.732049  ...   \n",
       "std                0.252807            0.442907  ...   \n",
       "min                0.000000            0.000000  ...   \n",
       "25%                0.000000            0.000000  ...   \n",
       "50%                0.000000            1.000000  ...   \n",
       "75%                0.000000            1.000000  ...   \n",
       "max                1.000000            1.000000  ...   \n",
       "\n",
       "       native-country_ Portugal  native-country_ Puerto-Rico  \\\n",
       "count              15055.000000                 15055.000000   \n",
       "mean                   0.001860                     0.004384   \n",
       "std                    0.043087                     0.066068   \n",
       "min                    0.000000                     0.000000   \n",
       "25%                    0.000000                     0.000000   \n",
       "50%                    0.000000                     0.000000   \n",
       "75%                    0.000000                     0.000000   \n",
       "max                    1.000000                     1.000000   \n",
       "\n",
       "       native-country_ Scotland  native-country_ South  \\\n",
       "count              15055.000000           15055.000000   \n",
       "mean                   0.000598               0.001993   \n",
       "std                    0.024444               0.044597   \n",
       "min                    0.000000               0.000000   \n",
       "25%                    0.000000               0.000000   \n",
       "50%                    0.000000               0.000000   \n",
       "75%                    0.000000               0.000000   \n",
       "max                    1.000000               1.000000   \n",
       "\n",
       "       native-country_ Taiwan  native-country_ Thailand  \\\n",
       "count            15055.000000              15055.000000   \n",
       "mean                 0.000864                  0.000797   \n",
       "std                  0.029374                  0.028222   \n",
       "min                  0.000000                  0.000000   \n",
       "25%                  0.000000                  0.000000   \n",
       "50%                  0.000000                  0.000000   \n",
       "75%                  0.000000                  0.000000   \n",
       "max                  1.000000                  1.000000   \n",
       "\n",
       "       native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
       "count                     15055.000000                   15055.000000   \n",
       "mean                          0.000531                       0.915510   \n",
       "std                           0.023046                       0.278131   \n",
       "min                           0.000000                       0.000000   \n",
       "25%                           0.000000                       1.000000   \n",
       "50%                           0.000000                       1.000000   \n",
       "75%                           0.000000                       1.000000   \n",
       "max                           1.000000                       1.000000   \n",
       "\n",
       "       native-country_ Vietnam  native-country_ Yugoslavia  \n",
       "count             15055.000000                15055.000000  \n",
       "mean                  0.001262                    0.000465  \n",
       "std                   0.035504                    0.021559  \n",
       "min                   0.000000                    0.000000  \n",
       "25%                   0.000000                    0.000000  \n",
       "50%                   0.000000                    0.000000  \n",
       "75%                   0.000000                    0.000000  \n",
       "max                   1.000000                    1.000000  \n",
       "\n",
       "[8 rows x 103 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistic \n",
    "test_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into numpy array\n",
    "Y_train = train_df['label'].to_numpy()\n",
    "Y_test = test_df['label'].to_numpy()\n",
    "X_train = train_df.drop(['label'], axis=1).to_numpy()\n",
    "X_test = test_df.drop(['label'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.2\n",
    "Derive the gradient and hessian matrix for the new E(w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient matrix for E(w)\n",
    "\n",
    "assume that $z_1 = tln\\sigma(w^T\\phi)$, $z_2 = (1-t)ln[1-\\sigma(w^T\\phi)]$\n",
    "\n",
    "$\\nabla E(w) = \\Lambda w + \\frac{\\partial z_1}{\\partial w} + \\frac{\\partial z_2}{\\partial w}$\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial w} = t[1-\\phi(w^T\\phi)\\phi]$\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial w} = (t-1)\\sigma(w^T\\phi)\\phi$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w} = \\frac{\\partial z_1}{\\partial w} + \\frac{\\partial z_2}{\\partial w} = (t-\\sigma(w^T\\phi))\\phi$\n",
    "\n",
    "So $\\nabla E(w)$ can be derived as $\\Lambda w - (t-\\sigma(w^T\\phi) = \\Lambda w + \\phi^T(y-t)$\n",
    "\n",
    "## Hessian matrix for E(w)\n",
    "\n",
    "The gradient matrix for $E(w)$ is $\\nabla E(w) = \\Lambda w + \\phi^T(y-t)$, and $\\frac{\\partial z}{\\partial w} = \\phi^T(y-t) =  \\sum _{n=1}^{N} (y_n-t_n)\\phi_n$.\n",
    "\n",
    "The Hessian matrix for $z$ can be derived as $\\sum _{n=1}^{N} y_n(1-y_n)\\phi_n \\phi_n^T = \\Phi^T R \\Phi$, which $R = y_n(1-y_n)$.\n",
    "\n",
    "So the $\\nabla \\nabla E(x)$ can be derived as $\\Lambda I + \\Phi^T R \\Phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form solution for ridge regression\n",
    "def closed_form_reg_solution(X, t, b): \n",
    "    n, m = X.shape\n",
    "    I = np.eye((m))\n",
    "    return (np.linalg.inv(X.T @ X + b * I) @ X.T @ t)\n",
    "\n",
    "# Y sigmoid function\n",
    "def Y_sigmoid_func(w, X):\n",
    "    return 1 / (1 + np.exp(-w.T @ X.T))\n",
    "\n",
    "# gradient matrix of E(w)\n",
    "def gradient_Ew_func(w, lamda, X, t, Y):\n",
    "    return lamda @ w + (X.T @ (Y-t))\n",
    "\n",
    "# hessian matrix of E(w)\n",
    "def hassian_Ew_func(X, Y, lamda):\n",
    "    n, m = X.shape\n",
    "    I_n = np.eye((n))\n",
    "    I_m = np.eye((m))\n",
    "    R = (Y * (1-Y)).T * I_n\n",
    "    return X.T @ R @ X + (lamda @ I_m)\n",
    "\n",
    "# E(w) error function\n",
    "def Ew_func(w, Y, t, lamda):\n",
    "    return 0.5 * w.T @ lamda @ w - t @ np.log(Y) + (1- t @ (np.log(1 - Y)))\n",
    "\n",
    "# new w\n",
    "def new_weight_func(w, X, lamda, t):\n",
    "    Y = Y_sigmoid_func(w, X)\n",
    "    gradient_Ew = gradient_Ew_func(w, lamda, X, t, Y)\n",
    "    hassian_Ew = hassian_Ew_func(X, Y, lamda)\n",
    "    w_new = w - np.linalg.inv(hassian_Ew) @ gradient_Ew\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "t = Y_train\n",
    "b = 1\n",
    "lamda = np.eye((X.shape[1]))\n",
    "\n",
    "w = closed_form_reg_solution(X, t, b)\n",
    "w_new = new_weight_func(w, X, lamda, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.3\n",
    "Create your mylogistic_l2 class. Show the learned 𝑤 as well as test accuracy for the cases below. If 𝑤 is too long for you, show selected 𝑤 for continuous-valued, binary-valued, and the constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter, tol, add_intercept):\n",
    "        self.reg_vec = reg_vec\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "    \n",
    "    def get_weights(self):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((self.X_train.shape[0], 1))\n",
    "            self.X_train = np.hstack((self.X_train, intercept))\n",
    "        \n",
    "        n, m = X_train.shape\n",
    "        I_n = np.eye((n))\n",
    "        I_m = np.eye((m))\n",
    "        \n",
    "        lamda_total = 0\n",
    "        for i in range(self.reg_vec.shape[0]):\n",
    "            lamda_total += self.reg_vec[i][i]\n",
    "        b = lamda_total / self.reg_vec.shape[0]\n",
    "        \n",
    "        weights = closed_form_reg_solution(self.X_train, self.Y_train, b)      \n",
    "        for step in range(self.max_iter):\n",
    "            Y = Y_sigmoid_func(weights, self.X_train)\n",
    "            \n",
    "            # calculate old weight error\n",
    "            old_error = Ew_func(weights, Y, self.Y_train, self.reg_vec)\n",
    "            weights = new_weight_func(weights, self.X_train, self.reg_vec, self.Y_train)\n",
    "            new_error = Ew_func(weights, Y, self.Y_train, self.reg_vec)\n",
    "            \n",
    "            #tol = abs(new_error - old_error) / old_error\n",
    "            tol = abs(new_error - old_error)\n",
    "            \n",
    "            if tol < self.tol:\n",
    "                break\n",
    "        return weights\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X_test.shape[0], 1))\n",
    "            X_test = np.hstack((X_test, intercept))\n",
    "        weights = self.get_weights()\n",
    "        #print(weights)\n",
    "        Y = weights.T @ X_test.T\n",
    "        return np.where(Y < 0.5, 0, 1)\n",
    "\n",
    "def accuracy_func(ypred, Y_test):\n",
    "    correct = 0\n",
    "    length = ypred.shape[0]\n",
    "    for i in range(length):\n",
    "        if ypred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return correct / length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1 \n",
    "lambda = 1 for all coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec1 = np.eye((X_train.shape[1]))\n",
    "\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec1, max_iter = 1000, tol = 1e-5, add_intercept = False)\n",
    "logic1.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.41796566e-02  7.82707987e-07  1.08400372e-01  3.27463928e-04\n",
      "  6.53736607e-04  2.87834834e-02 -9.76323532e-02 -8.03641945e-01\n",
      " -6.01053744e-01 -4.80847877e-01 -1.08487923e+00 -9.26905393e-01\n",
      " -1.08986922e+00 -7.90571297e-01 -9.11950199e-01 -5.68761340e-01\n",
      " -6.73786320e-01 -7.05811615e-01 -1.26228488e+00 -8.62519888e-01\n",
      " -2.24963151e-01 -1.71046728e-01  2.62067023e-01  9.22538483e-01\n",
      " -4.23305027e-01  5.22291321e-01 -9.58284273e-01  9.73254244e-01\n",
      " -2.11696115e-01 -1.28689242e+00  1.03199860e+00  4.53675532e-01\n",
      " -1.14890618e+00 -1.80071377e+00 -1.25888819e+00 -1.07510332e+00\n",
      " -2.66283185e-01 -2.35635588e-01  5.28024283e-01 -1.23731090e+00\n",
      " -9.89154176e-01 -5.66780806e-01 -1.14193057e+00 -1.78279101e+00\n",
      "  2.42865945e-01  3.05633246e-01  2.78021462e-02  4.30173678e-01\n",
      " -3.99442820e-01 -7.98477717e-01 -7.16337180e-01 -1.35346836e+00\n",
      " -1.82462710e+00 -8.94856329e-01  5.02936922e-01 -1.28483941e+00\n",
      " -7.15535839e-01 -1.04896648e+00 -1.14020777e+00 -8.95280255e-01\n",
      " -2.96597444e+00 -2.11885532e+00  8.07936994e-01  3.66592133e-01\n",
      " -5.76208581e-01 -1.49351021e+00  3.52190978e-01 -9.34978835e-01\n",
      " -1.83727316e-01 -3.75602483e-01  1.36906350e-01  4.21541248e-01\n",
      "  3.50770517e-01 -7.77705688e-01 -1.60377695e-01  4.93200878e-02\n",
      " -1.62352879e-01 -7.88936025e-02 -3.47373000e-02 -3.28184336e-01\n",
      "  3.90244184e-03  4.31272427e-01  8.21308543e-01  2.00075368e-01\n",
      "  3.84571006e-01 -5.38140085e-01 -4.09701989e-01 -6.76940647e-01\n",
      " -4.93886376e-01 -4.68899006e-01  3.23703874e-01  5.33369929e-02\n",
      "  1.10690428e-01 -3.43595731e-01 -4.02683804e-01 -1.00385373e+00\n",
      " -3.20165201e-01 -4.37936597e-01 -1.24271484e-01  2.42901834e-01\n",
      " -6.50616961e-01  8.35119556e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8418465626037861"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_1 = logic1.predict(X_test)\n",
    "accuracy_func(ypred_1, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "lambda = 1 for all but the intercept, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.45446275e-02  7.98282217e-07  1.67756907e-01  3.27738403e-04\n",
      "  6.54331881e-04  2.90881427e-02  1.99256461e-01 -5.06096798e-01\n",
      " -3.00440222e-01 -1.85659222e-01 -7.87953042e-01 -6.30096032e-01\n",
      " -9.56341943e-01 -4.97867988e-01 -6.75858090e-01 -3.94972376e-01\n",
      " -2.30340427e-01 -2.74589294e-01 -8.62126708e-01 -5.20861517e-01\n",
      " -2.76481320e-01 -1.64246632e-01  1.51433614e-01  6.40531797e-01\n",
      " -2.99144069e-01  3.52792214e-01 -7.17679155e-01  7.48089987e-01\n",
      " -1.46010834e-01 -1.00841211e+00  1.26871756e+00  7.59444064e-01\n",
      " -8.81298895e-01 -1.51819614e+00 -9.84334351e-01 -8.03250919e-01\n",
      " -1.13265116e-01 -8.19790035e-02  6.79518510e-01 -1.08769344e+00\n",
      " -8.36301526e-01 -4.11922763e-01 -9.87562799e-01 -1.69486500e+00\n",
      "  3.93865848e-01  4.58291164e-01  1.79115384e-01  5.82023328e-01\n",
      " -2.46555383e-01 -4.91873716e-01 -3.85008462e-01 -1.04170801e+00\n",
      " -1.49916238e+00 -5.62758486e-01  8.13180258e-01 -9.10717464e-01\n",
      " -3.19372946e-01 -6.58777065e-01 -7.74795583e-01 -5.03667739e-01\n",
      " -2.00734362e+00 -1.15998718e+00  8.76322973e-01  4.22548664e-01\n",
      " -5.24456015e-01 -1.45263765e+00  4.09456385e-01 -8.85389454e-01\n",
      " -1.37202042e-01 -3.17540941e-01  1.91331439e-01  4.68910834e-01\n",
      "  4.08342560e-01 -7.30912631e-01 -1.13013373e-01  9.53405904e-02\n",
      " -1.50639289e-01 -3.72439664e-02  3.46075414e-03 -2.71470885e-01\n",
      "  5.67640193e-02  4.74814021e-01  8.82944376e-01  2.54422989e-01\n",
      "  4.34467484e-01 -5.03798582e-01 -3.31495510e-01 -6.35603883e-01\n",
      " -4.74055240e-01 -4.30322602e-01  3.80381165e-01  1.08192432e-01\n",
      "  1.65617892e-01 -2.81968848e-01 -3.73291282e-01 -9.56701177e-01\n",
      " -2.72119877e-01 -4.06192686e-01 -8.97096435e-02  3.05354789e-01\n",
      " -6.05121976e-01  8.74883387e-01 -3.16733080e+00]\n"
     ]
    }
   ],
   "source": [
    "lambda_vec2 = np.eye((X_train.shape[1]+1))\n",
    "\n",
    "logic2 = mylogistic_l2(reg_vec = lambda_vec2, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic2.fit(X_train, Y_train)\n",
    "ypred_2 = logic2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8419129857190302"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_func(ypred_2, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3\n",
    "lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.51943584e-02  8.25079205e-07  2.73123474e-01  3.28484653e-04\n",
      "  6.55896509e-04  2.96242121e-02  7.49673973e-01  4.14197651e-02\n",
      "  2.53152824e-01  3.60260306e-01 -2.40375315e-01 -8.42326581e-02\n",
      " -1.07989890e+00  2.48623967e-02 -2.54945992e-01 -8.49103770e-02\n",
      "  5.97334592e-01  5.19136868e-01 -1.50775661e-01  9.48662461e-02\n",
      " -3.68894364e-01 -1.50016575e-01 -4.26772078e-02  1.45384620e-01\n",
      " -7.54976301e-02  5.54830533e-02 -6.39419836e-01  3.56934143e-01\n",
      " -2.68642754e-02 -5.96666351e-01  1.88844897e+00  1.28796019e+00\n",
      " -4.95569350e-01 -1.09875499e+00 -5.82320923e-01 -4.03097552e-01\n",
      "  1.80015776e-01  2.12404103e-01  9.71987214e-01 -8.08700111e-01\n",
      " -5.48903334e-01 -1.15233855e-01 -6.98705781e-01 -2.03017272e+00\n",
      "  6.84579948e-01  7.56105130e-01  4.70939024e-01  8.78937484e-01\n",
      "  4.67471251e-02 -2.56247513e-02  1.94024147e-01 -5.33943211e-01\n",
      " -9.49276327e-01  2.00222635e-02  1.29479788e+00 -3.04493145e-01\n",
      "  3.54270212e-01 -1.77715865e-02 -1.71000238e-01  1.38994758e-01\n",
      " -4.26014303e-01  4.26014303e-01  1.16013190e+00  5.71649766e-01\n",
      " -4.55844256e-01 -1.68610032e+00  5.67441670e-01 -9.86156265e-01\n",
      " -4.59705891e-02 -2.12307450e-01  3.31240987e-01  6.47216691e-01\n",
      "  5.58978093e-01 -7.14479935e-01 -1.26023717e-02  2.36316448e-01\n",
      " -2.17265640e-01  4.92044253e-02  1.10766370e-01 -1.72201048e-01\n",
      "  1.88253978e-01  6.72920719e-01  1.07237944e+00  4.11321391e-01\n",
      "  5.78675180e-01 -5.78167214e-01 -1.76575775e-01 -7.13298999e-01\n",
      " -6.81370156e-01 -4.29115638e-01  5.02841215e-01  2.51743444e-01\n",
      "  3.28101693e-01 -1.63861932e-01 -4.20912963e-01 -9.43347678e-01\n",
      " -1.98369173e-01 -4.38311182e-01 -2.26657348e-02  4.46198096e-01\n",
      " -5.92571837e-01  1.17611465e+00 -8.60427623e+00]\n"
     ]
    }
   ],
   "source": [
    "lambda3_list = []\n",
    "for col in new_features_list:\n",
    "    # lambda = 1 for numerical-valued features\n",
    "    if col in original_features_list:\n",
    "        lambda3_list.append(1)\n",
    "    else:\n",
    "        lambda3_list.append(0.5)\n",
    "\n",
    "lambda3_list.append(0)\n",
    "lambda_vec3 = np.diag(np.array(lambda3_list))\n",
    "\n",
    "logic3 = mylogistic_l2(reg_vec = lambda_vec3, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic3.fit(X_train, Y_train)\n",
    "ypred_3 = logic3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8418465626037861"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_func(ypred_3, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.4 \n",
    "Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let 𝑎1 and 𝑎2 denote the regularization coefficients for continuous-valued and binary-valued features. Search the best 𝑎1 and 𝑎2 and report the test accuracy using the best hyper-parameters. You should follow the procedure to search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into subtraining and tunning\n",
    "X_train, X_tuning, Y_train, Y_tunning = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "# Choose a set of grids among a reasonable range.\n",
    "grids = [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct grid search with the constraint that 𝑎1=𝑎2\n",
    "best_accuracy = 0\n",
    "for g in grids:\n",
    "    lambda_list = []\n",
    "    for col in new_features_list:\n",
    "        lambda_list.append(g)\n",
    "    lambda_list.append(0)\n",
    "    lambda_vec_tunning1 = np.diag(np.array(lambda_list))\n",
    "    logic_tunning1 = mylogistic_l2(reg_vec = lambda_vec_tunning1, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic_tunning1.fit(X_train, Y_train)\n",
    "    ypred_tunning1 = logic_tunning1.predict(X_tuning)\n",
    "    accuracy = accuracy_func(ypred_tunning1, Y_tunning)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_a = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix  𝑎1=𝑎∗1, and search 𝑎2 for the best value, call the result the new 𝑎∗2\n",
    "best_accuracy = 0\n",
    "a1 = best_a\n",
    "a2_star = 0\n",
    "\n",
    "for g in grids:\n",
    "    lambda_list = []\n",
    "    for col in new_features_list:\n",
    "        # a1 for numerical-valued features\n",
    "        if col in original_features_list:\n",
    "            lambda_list.append(a1)\n",
    "        else:\n",
    "            lambda_list.append(g)\n",
    "    lambda_list.append(0)\n",
    "    lambda_vec_tunning2 = np.diag(np.array(lambda_list))\n",
    "    \n",
    "    logic_tunning2 = mylogistic_l2(reg_vec = lambda_vec_tunning2, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic_tunning2.fit(X_train, Y_train)\n",
    "    ypred_tunning2 = logic_tunning2.predict(X_tuning)\n",
    "    accuracy = accuracy_func(ypred_tunning1, Y_tunning)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        a2_star = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix  𝑎2=𝑎∗2, and search 𝑎1 for the best value. Report the selected 𝑎1 and  𝑎2.\n",
    "best_accuracy = 0\n",
    "a1_star = 0\n",
    "\n",
    "for g in grids:\n",
    "    lambda_list = []\n",
    "    for col in new_features_list:\n",
    "        # a1 for numerical-valued features\n",
    "        if col in original_features_list:\n",
    "            lambda_list.append(g)\n",
    "        else:\n",
    "            lambda_list.append(a2_star)\n",
    "    lambda_list.append(0)\n",
    "    lambda_vec_tunning3 = np.diag(np.array(lambda_list))\n",
    "    \n",
    "    logic_tunning3 = mylogistic_l2(reg_vec = lambda_vec_tunning3, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic_tunning3.fit(X_train, Y_train)\n",
    "    ypred_tunning3 = logic_tunning3.predict(X_tuning)\n",
    "    accuracy = accuracy_func(ypred_tunning3, Y_tunning)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        a1_star = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8410494852208569\n"
     ]
    }
   ],
   "source": [
    "# Train a model using the selected hyper-parameters, and report the test accuracy.\n",
    "lambda_list = []\n",
    "for col in new_features_list:\n",
    "    # a1 for numerical-valued features\n",
    "    if col in original_features_list:\n",
    "        lambda_list.append(a1_star)\n",
    "    else:\n",
    "        lambda_list.append(a2_star)\n",
    "lambda_list.append(0)\n",
    "lambda_vec_tunning4 = np.diag(np.array(lambda_list))\n",
    "\n",
    "logic_tunning4 = mylogistic_l2(reg_vec = lambda_vec_tunning4, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic_tunning4.fit(X_train, Y_train)\n",
    "ypred_tunning4 = logic_tunning4.predict(X_test)\n",
    "accuracy = accuracy_func(ypred_tunning4, Y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "print(a1_star)\n",
    "print(a2_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.5 \n",
    "Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347392892726669\n",
      "0.8160743938890734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844968449020259\n",
      "0.8176021255396878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160743938890734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8444370640983062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844902025905015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8439056791763534\n",
      "0.8187313184988376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/michelle/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8180670873463965\n",
      "estimated parameters:  0.1 , accuracy:  0.844968449020259\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_g = 0\n",
    "for g in grids:\n",
    "    logic_tunning5 = LogisticRegression(C = g, tol = 1e-5, max_iter = 1000)\n",
    "    logic_tunning5.fit(X_train, Y_train)\n",
    "    accuracy = logic_tunning5.score(X_tuning, Y_tuning)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_g = g\n",
    "\n",
    "logic_tunning5_final = LogisticRegression(C = best_g, tol = 1e-5, max_iter = 1000)\n",
    "logic_tunning5_final.fit(X_train, Y_train)\n",
    "accuracy_final = logic_tunning5.score(X_test, Y_test)\n",
    "print('estimated parameters: ', best_g, ', accuracy: ', accuracy_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "sklearn.linear_model.LogisticRegression 做完 hyperparameter tuning 後 predict 出的最佳 accuracy 為 0.844968449020259、regularization 為 0.1。\n",
    "\n",
    "而 mylogistic_l2 做完 hyperparameter tuning 後 predict 出的 accuracy 為 0.8410494852208569，在此 training data 中 tun 出的 continuous-valued features regularization 為 0.01、binary-valued features regularization 同樣為 0.01。\n",
    "\n",
    "雖然 sklearn.linear_model.LogisticRegression predict 出的 accuracy 比 mylogistic_l2 分開 continuous-valued features regularization 與 binary-valued features regularization 時 predict 出的 accuracy 高，但是同樣用 mylogistic_l2 進行 predict 時可發現，分開 regularization 的 accuracy 比兩種 features 使用同一 regularization 時高，並且加上 intercept 後也可看出 accuracy (0.8419129857190302) 比沒有加 intercept (0.8418465626037861) 來得高。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
