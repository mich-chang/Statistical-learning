{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, np, npx, nd, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import eval_measures\n",
    "import random\n",
    "import numpy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "with open('msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test']\n",
    "\n",
    "# reserve the last 10% of the training dataset as the validation set, and the remaining 90% as the subtraining set.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y de mean\n",
    "Y_train_mean = Y_train.mean()\n",
    "de_mean_Y_train = Y_train - Y_train_mean\n",
    "de_mean_Y_test = Y_test - Y_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 1000\n",
    "X_train_10000 = X_train[:10000]\n",
    "Y_train_10000 = Y_train[:10000]\n",
    "de_mean_Y_train_10000 = de_mean_Y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "Train and tune the models listed above. Report test RMSE for each model setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w_list):\n",
    "    total = 0\n",
    "    for w in w_list:\n",
    "        total = total + (w**2).sum() / 2\n",
    "    return total / len(w_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009.9896859926644\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(Y_train_10000,X_train_10000).fit()\n",
    "y_pred = model.predict(X_val)\n",
    "ols_rmse = eval_measures.rmse(Y_val, y_pred)\n",
    "rmse_dict['OLS'] = ols_rmse\n",
    "print(ols_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 1000\n",
    "X_train_10000 = np.array(X_train_10000)\n",
    "Y_train_10000 = np.array(Y_train_10000)\n",
    "de_mean_Y_train_10000 = np.array(de_mean_Y_train_10000)\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = np.array(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case MLP_0_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal(0, 0.01, (90, 1)).astype('float64')\n",
    "b = np.zeros(1)\n",
    "w.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 4  # Number of iterations\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training dataset are used once in one epoch\n",
    "    # iteration. The features and tags of minibatch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, X_train_10000, de_mean_Y_train_10000):\n",
    "        with autograd.record():\n",
    "            l = squared_loss(linreg(X, w, b), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w, b]\n",
    "        SGD([w, b], lr, batch_size)  # Update parameters using their gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'rmse': 1998.4381879230896}\n"
     ]
    }
   ],
   "source": [
    "y_pred = linreg(X_val, w, b)\n",
    "loss = squared_loss(linreg(X_val, w, b), Y_val)\n",
    "\n",
    "mlp_0_dm_rmse = mx.metric.RMSE()\n",
    "mlp_0_dm_rmse.update(labels = Y_val, preds = y_pred)\n",
    "print(mlp_0_dm_rmse)\n",
    "rmse_dict['mlp_0_dm'] = mlp_0_dm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case MLP_1_dm\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = X_train_10000.shape[1], 1, 45\n",
    "\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens)).astype('float64')\n",
    "b1 = np.zeros(num_hiddens)\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs)).astype('float64')\n",
    "b2 = np.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_1_net(X):\n",
    "    X = X.reshape(-1, num_inputs)\n",
    "    H = relu(np.dot(X, W1) + b1)\n",
    "    return np.dot(H, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 4  # Number of iterations\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training dataset are used once in one epoch\n",
    "    # iteration. The features and tags of minibatch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, X_train_10000, de_mean_Y_train_10000):\n",
    "        with autograd.record():\n",
    "            l = squared_loss(mlp_1_net(X), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w, b]\n",
    "        SGD([W1, b1, W2, b2], lr, batch_size)  # Update parameters using their gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'rmse': 1998.4392694005303}\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp_1_net(X_val)\n",
    "loss = squared_loss(mlp_1_net(X_val), Y_val)\n",
    "\n",
    "mlp_1_dm_rmse = mx.metric.RMSE()\n",
    "mlp_1_dm_rmse.update(labels = Y_val, preds = y_pred)\n",
    "print(mlp_1_dm_rmse)\n",
    "rmse_dict['mlp_1_dm'] = mlp_1_dm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case MLP_2_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = X_train_10000.shape[1], 1, 45\n",
    "\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens)).astype('float64')\n",
    "b1 = np.zeros(num_hiddens)\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_hiddens)).astype('float64')\n",
    "b2 = np.zeros(num_outputs)\n",
    "W3 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs)).astype('float64')\n",
    "b3 = np.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_2_net(X):\n",
    "    X = X.reshape(-1, num_inputs)\n",
    "    H1 = relu(np.dot(X, W1) + b1)\n",
    "    H2 = relu(np.dot(H1, W2) + b2)\n",
    "    return np.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 4  # Number of iterations\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training dataset are used once in one epoch\n",
    "    # iteration. The features and tags of minibatch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, X_train_10000, de_mean_Y_train_10000):\n",
    "        with autograd.record():\n",
    "            l = squared_loss(mlp_2_net(X), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w, b]\n",
    "        SGD([W1, b1, W2, b2, W3, b3], lr, batch_size)  # Update parameters using their gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'rmse': 1998.4379025121495}\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp_2_net(X_val)\n",
    "loss = squared_loss(linreg(X_val, w, b), Y_val)\n",
    "\n",
    "mlp_2_dm_rmse = mx.metric.RMSE()\n",
    "mlp_2_dm_rmse.update(labels = Y_val, preds = y_pred)\n",
    "print(mlp_2_dm_rmse)\n",
    "rmse_dict['mlp_2_dm'] = mlp_2_dm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case MLP_2_dm_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens)).astype('float64')\n",
    "    b1 = np.zeros(num_hiddens)\n",
    "    W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_hiddens)).astype('float64')\n",
    "    b2 = np.zeros(num_outputs)\n",
    "    W3 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs)).astype('float64')\n",
    "    b3 = np.zeros(num_outputs)\n",
    "    params = [W1, b1, W2, b2, W3, b3]\n",
    "    \n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-104-80a875f0cfbc>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-104-80a875f0cfbc>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    mlp_2_dm_l2_rmse = mx.metric.RMSE()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_l2(lambd):\n",
    "    W1, b1, W2, b2, W3, b3 = init_params()\n",
    "    net, loss = lambda X: mlp_2_net(X), squared_loss\n",
    "    num_epochs, lr = 4, 0.003\n",
    "    #animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n",
    "    #                        xlim=[1, num_epochs], legend=['train', 'test'])\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter(batch_size, X_train_10000, de_mean_Y_train_10000):\n",
    "            with autograd.record():\n",
    "                # The L2 norm penalty term has been added, and broadcasting\n",
    "                # makes l2_penalty(w) a vector whose length is batch_size\n",
    "                l = loss(net(X), y) + lambd * l2_penalty([W1, W2, W3])\n",
    "            l.backward()\n",
    "            SGD([W1, b1, W2, b2, W3, b3], lr, batch_size)\n",
    "        #if epoch % 5 == 0:\n",
    "        #    animator.add(epoch, (d2l.evaluate_loss(net, train_iter, loss),\n",
    "        #                         d2l.evaluate_loss(net, test_iter, loss)))\n",
    "    y_pred = mlp_2_net(X_val)\n",
    "    loss = squared_loss(mlp_2_net(X_val)\n",
    "    mlp_2_dm_l2_rmse = mx.metric.RMSE()\n",
    "    mlp_2_dm_l2_rmse.update(labels = Y_val, preds = y_pred)\n",
    "    print(mlp_2_dm_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10000 = np.array(X_train_10000, dtype='float32')\n",
    "Y_train_10000 = np.array(Y_train_10000, dtype='float32')\n",
    "de_mean_Y_train_10000 = np.array(de_mean_Y_train_10000, dtype='float32')\n",
    "X_val = np.array(X_val, dtype='float32')\n",
    "Y_val = np.array(Y_val, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gluon(wd):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=1))\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    num_epochs, lr = 4, 0.003\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'wd': wd})\n",
    "    # The bias parameter has not decayed. Bias names generally end with \"bias\"\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "    #animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n",
    "    #                        xlim=[1, num_epochs], legend=['train', 'test'])\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        for X, y in data_iter(batch_size, X_train_10000, de_mean_Y_train_10000):\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        #if epoch % 5 == 0:\n",
    "        #    animator.add(epoch, (d2l.evaluate_loss(net, train_iter, loss),\n",
    "        #                         d2l.evaluate_loss(net, test_iter, loss)))\n",
    "    print('L1 norm of w:', np.abs(net[0].weight.data()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm of w: "
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[13:48:22] include/mxnet/./tensor_blob.h:255: Check failed: mshadow::DataType<DType>::kFlag == type_flag_: TBlob.get_with_shape: data type do not match specified type.Expected: 0 v.s. given 1\nStack trace:\n  [bt] (0) 1   libmxnet.so                         0x0000001a171c0bd9 libmxnet.so + 31705\n  [bt] (1) 2   libmxnet.so                         0x0000001a171d0f1c mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 44396\n  [bt] (2) 3   libmxnet.so                         0x0000001a1837406a void mxnet::op::BinaryBroadcastCsrDnsDnsImpl<mshadow::cpu, mxnet::op::mshadow_op::plus>(mxnet::OpContext const&, mxnet::NDArray const&, mxnet::NDArray const&, mxnet::OpReqType, mxnet::NDArray const&, mxnet::TShape const&, mxnet::TShape const&, mxnet::TShape const&, int, bool) + 33274\n  [bt] (3) 4   libmxnet.so                         0x0000001a1835dfd9 void mxnet::op::BinaryBroadcastComputeDenseEx<mshadow::cpu, mxnet::op::mshadow_op::plus>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&) + 4361\n  [bt] (4) 5   libmxnet.so                         0x0000001a18f408b7 mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext)::operator()(mxnet::RunContext) const + 695\n  [bt] (5) 6   libmxnet.so                         0x0000001a18f3fd6d std::__1::__function::__func<mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext), std::__1::allocator<mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext)>, void (mxnet::RunContext)>::operator()(mxnet::RunContext&&) + 29\n  [bt] (6) 7   libmxnet.so                         0x0000001a18e9d3e8 dmlc::ThreadLocalStore<mxnet::engine::ThreadedEngine::BulkStatus>::Get() + 7208\n  [bt] (7) 8   libmxnet.so                         0x0000001a18ea2b75 dmlc::ThreadLocalStore<mxnet::engine::ThreadedEngine::BulkStatus>::Get() + 29621\n  [bt] (8) 9   libmxnet.so                         0x0000001a18ea635e std::__1::shared_ptr<mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0> > mxnet::common::LazyAllocArray<mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0> >::Get<mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::'lambda2'()>(int, mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::'lambda2'()) + 3166\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-cd1136ac029d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_gluon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-a110d5831673>\u001b[0m in \u001b[0;36mtrain_gluon\u001b[0;34m(wd)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#    animator.add(epoch, (d2l.evaluate_loss(net, train_iter, loss),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#                         d2l.evaluate_loss(net, test_iter, loss)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L1 norm of w:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/numpy/multiarray.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"\"\"Returns a string representation of the array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0marray_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cpu'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \"\"\"\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [13:48:22] include/mxnet/./tensor_blob.h:255: Check failed: mshadow::DataType<DType>::kFlag == type_flag_: TBlob.get_with_shape: data type do not match specified type.Expected: 0 v.s. given 1\nStack trace:\n  [bt] (0) 1   libmxnet.so                         0x0000001a171c0bd9 libmxnet.so + 31705\n  [bt] (1) 2   libmxnet.so                         0x0000001a171d0f1c mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 44396\n  [bt] (2) 3   libmxnet.so                         0x0000001a1837406a void mxnet::op::BinaryBroadcastCsrDnsDnsImpl<mshadow::cpu, mxnet::op::mshadow_op::plus>(mxnet::OpContext const&, mxnet::NDArray const&, mxnet::NDArray const&, mxnet::OpReqType, mxnet::NDArray const&, mxnet::TShape const&, mxnet::TShape const&, mxnet::TShape const&, int, bool) + 33274\n  [bt] (3) 4   libmxnet.so                         0x0000001a1835dfd9 void mxnet::op::BinaryBroadcastComputeDenseEx<mshadow::cpu, mxnet::op::mshadow_op::plus>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&) + 4361\n  [bt] (4) 5   libmxnet.so                         0x0000001a18f408b7 mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext)::operator()(mxnet::RunContext) const + 695\n  [bt] (5) 6   libmxnet.so                         0x0000001a18f3fd6d std::__1::__function::__func<mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext), std::__1::allocator<mxnet::imperative::PushFCompute(std::__1::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&, std::__1::vector<mxnet::TBlob, std::__1::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::engine::Var*, std::__1::allocator<mxnet::engine::Var*> > const&, std::__1::vector<mxnet::Resource, std::__1::allocator<mxnet::Resource> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<unsigned int, std::__1::allocator<unsigned int> > const&, std::__1::vector<mxnet::OpReqType, std::__1::allocator<mxnet::OpReqType> > const&)::'lambda'(mxnet::RunContext)>, void (mxnet::RunContext)>::operator()(mxnet::RunContext&&) + 29\n  [bt] (6) 7   libmxnet.so                         0x0000001a18e9d3e8 dmlc::ThreadLocalStore<mxnet::engine::ThreadedEngine::BulkStatus>::Get() + 7208\n  [bt] (7) 8   libmxnet.so                         0x0000001a18ea2b75 dmlc::ThreadLocalStore<mxnet::engine::ThreadedEngine::BulkStatus>::Get() + 29621\n  [bt] (8) 9   libmxnet.so                         0x0000001a18ea635e std::__1::shared_ptr<mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0> > mxnet::common::LazyAllocArray<mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0> >::Get<mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::'lambda2'()>(int, mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::'lambda2'()) + 3166\n\n"
     ]
    }
   ],
   "source": [
    "train_gluon(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
